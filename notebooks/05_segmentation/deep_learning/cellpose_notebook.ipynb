{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a268f5ed",
   "metadata": {},
   "source": [
    "# Cellpose in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3bd4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# /// script\n",
    "# requires-python = \">=3.12\"\n",
    "# dependencies = [\n",
    "#     \"matplotlib\",\n",
    "#     \"tifffile\",\n",
    "#     \"cellpose\"\n",
    "# ]\n",
    "# ///"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a63fe2e",
   "metadata": {},
   "source": [
    "## <mark style=\"color: black; background-color: rgb(127,196,125); padding: 3px; border-radius: 5px;\">Overview</mark>\n",
    "\n",
    "[Website](https://www.cellpose.org) | [GitHub](https://github.com/mouseland/cellpose) | [Paper](https://www.biorxiv.org/content/10.1101/2025.04.28.651001v1)\n",
    "\n",
    "In this section, weâ€™ll learn how to use **Cellpose**, a powerful deep learning tool for cell segmentation. Cellpose works on a wide variety of microscopy imagesâ€”including both nuclei and full cellsâ€”and doesnâ€™t require retraining for many common use cases.\n",
    "\n",
    "Weâ€™ll start by running Cellpose on a single image and then move to batch processing and visualization. Later, weâ€™ll explore how to customize parameters and interpret Cellpose outputs like masks and flow fields.\n",
    "\n",
    "> ðŸ’¡ Tip: If you're not using a GPU (or are on a Mac with Apple Silicon), we recommend running this notebook on **Google Colab** for faster performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f437ad73",
   "metadata": {},
   "source": [
    "## <mark style=\"color: black; background-color: rgb(127,196,125); padding: 3px; border-radius: 5px;\">Import Libraries</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d831fd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e6d9c3c",
   "metadata": {},
   "source": [
    "## <mark style=\"color: black; background-color: rgb(127,196,125); padding: 3px; border-radius: 5px;\">Setup</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3369f0fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a72afec2",
   "metadata": {},
   "source": [
    "## <mark style=\"color: black; background-color: rgb(127,196,125); padding: 3px; border-radius: 5px;\">Run Cellpose on A Sample Image</mark>\n",
    "\n",
    "In this section, weâ€™ll apply Cellpose to a single image and visualize the segmentation result.\n",
    "\n",
    "Youâ€™ll see how easy it is to segment cells using just a few lines of code. Weâ€™ll use the `model.eval()` function to make predictions, and then display:\n",
    "- The original image\n",
    "- The predicted segmentation mask\n",
    "- The outlines of detected cells\n",
    "\n",
    "Weâ€™ll also look at the `flows` output, which helps Cellpose group pixels into individual objects.\n",
    "\n",
    "No model training requiredâ€”just load, run, and view the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6139b9c",
   "metadata": {},
   "source": [
    "### <mark style=\"color: black; background-color: rgb(190,223,185); padding: 3px; border-radius: 5px;\">Load the Image</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc120648",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8de800cb",
   "metadata": {},
   "source": [
    "### <mark style=\"color: black; background-color: rgb(190,223,185); padding: 3px; border-radius: 5px;\">Initialize the Model</mark>\n",
    "\n",
    "To initialize Cellpose model we can use the [`models.CellposeModel()`](https://cellpose.readthedocs.io/en/latest/api.html#cellposemodel) class.\n",
    "\n",
    "Among the parameters we can set if we want to use `gpu` (if available) as well which **model** to use. The current default is Cellpose-SAM as `cpsam` (another example is `cyto3`, the previous U-Net-based model).\n",
    "\n",
    "If it is the first time you run this notebook, the model will be downloaded automatically. This may take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac7d865",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61ba06fe",
   "metadata": {},
   "source": [
    "### <mark style=\"color: black; background-color: rgb(190,223,185); padding: 3px; border-radius: 5px;\">Run Cellpose</mark>\n",
    "\n",
    "After initializing the model, we can run it on the image using the [`model.eval()`](https://cellpose.readthedocs.io/en/latest/api.html#id0) method.\n",
    "\n",
    "Here are some of the parameters you can change:\n",
    "\n",
    "* ***flow_threshold*** is  the  maximum  allowed  error  of  the  flows  for  each  mask.   The  default  is 0.4.\n",
    "    *  **Increase** this threshold if cellpose is not returning as many masks as youâ€™d expect (or turn off completely with 0.0)\n",
    "    *   **Decrease** this threshold if cellpose is returning too many ill-shaped masks.\n",
    "\n",
    "* ***cellprob_threshold*** determines proability that a detected object is a cell.   The  default  is 0.0.\n",
    "    *   **Decrease** this threshold if cellpose is not returning as many masks as youâ€™d expect or if masks are too small\n",
    "    *   **Increase** this threshold if cellpose is returning too many masks esp from dull/dim areas.\n",
    "\n",
    "* ***tile_norm_blocksize*** determines the size of blocks used for normalizing the image. The default is 0, which means the entire image is normalized together.\n",
    "  You may want to change this to 100-200 pixels if you have very inhomogeneous brightness across your image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989a751e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a9b3c09",
   "metadata": {},
   "source": [
    "### <mark style=\"color: black; background-color: rgb(190,223,185); padding: 3px; border-radius: 5px;\">Display the Results</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bfbd8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ecc88037",
   "metadata": {},
   "source": [
    "## <mark style=\"color: black; background-color: rgb(127,196,125); padding: 3px; border-radius: 5px;\">Run Cellpose on a Folder of Images</mark>\n",
    "\n",
    "Now that weâ€™ve seen how to run Cellpose on a single image, letâ€™s scale up and apply it to a **folder of images**. This is useful when you have an entire experiment or dataset that you want to segment automatically.\n",
    "\n",
    "Cellpose provides a convenient built-in function for this: `model.eval()` can be used with a folder path, and it will process all compatible image files in that folder.\n",
    "\n",
    "Weâ€™ll walk through how to:\n",
    "- Point Cellpose to your image directory\n",
    "- Choose the segmentation type (e.g., \"nuclei\" or \"cyto\")\n",
    "- Set output options like saving masks and outlines\n",
    "- Visualize a few results to make sure everything worked\n",
    "\n",
    "> ðŸ’¡ Tip: Make sure your images are in `.tif`, `.png`, or `.jpg` format.\n",
    "\n",
    "Letâ€™s get started with folder-based segmentation!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c62c4ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
