
<!DOCTYPE html>

<html data-content_root="../../../" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/><meta content="width=device-width, initial-scale=1" name="viewport"/>
<title>Deep Learning Methods — BoBiAC Book</title>
<script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
<!-- Loaded before other Sphinx assets -->
<link href="../../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link as="font" crossorigin="" href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" rel="preload" type="font/woff2"/>
<link href="../../../_static/pygments.css?v=03e43079" rel="stylesheet" type="text/css"/>
<link href="../../../_static/styles/sphinx-book-theme.css?v=eba8b062" rel="stylesheet" type="text/css"/>
<link href="../../../_static/togglebutton.css?v=13237357" rel="stylesheet" type="text/css"/>
<link href="../../../_static/copybutton.css?v=76b2166b" rel="stylesheet" type="text/css"/>
<link href="../../../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" rel="stylesheet" type="text/css"/>
<link href="../../../_static/sphinx-thebe.css?v=4fa983c6" rel="stylesheet" type="text/css"/>
<link href="../../../_static/sphinx-design.min.css?v=95c83b7e" rel="stylesheet" type="text/css"/>
<link href="../../../_static/styles/custom.css?v=dd2a99f2" rel="stylesheet" type="text/css"/>
<!-- Pre-loaded scripts that we'll load fully later -->
<link as="script" href="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" rel="preload"/>
<link as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" rel="preload"/>
<script src="../../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../../_static/documentation_options.js?v=9eb32ce0"></script>
<script src="../../../_static/doctools.js?v=9a2dae69"></script>
<script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
<script src="../../../_static/clipboard.min.js?v=a7894cd8"></script>
<script src="../../../_static/copybutton.js?v=f281be69"></script>
<script src="../../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
<script>let toggleHintShow = 'Click to show';</script>
<script>let toggleHintHide = 'Click to hide';</script>
<script>let toggleOpenOnPrint = 'true';</script>
<script src="../../../_static/togglebutton.js?v=4a39c7ea"></script>
<script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
<script src="../../../_static/design-tabs.js?v=f930bc37"></script>
<script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
<script async="async" src="../../../_static/sphinx-thebe.js?v=c100c467"></script>
<script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
<script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
<script>DOCUMENTATION_OPTIONS.pagename = 'content/05_segmentation/deep_learning/deep_learning';</script>
<script src="../../../_static/scripts/custom.js?v=4332ec77"></script>
<link href="../../../genindex.html" rel="index" title="Index"/>
<link href="../../../search.html" rel="search" title="Search"/>
<link href="intro_to_cellpose.html" rel="next" title="Introduction to Cellpose"/>
<link href="../machine_learning/from_ilastik_masks_to_labels.html" rel="prev" title="From Ilastik Masks to Labels"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="en" name="docsearch:language"/>
</head>
<body data-bs-root-margin="0px 0px -60%" data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-default-mode="" data-offset="180">
<div class="skip-link d-print-none" id="pst-skip-link"><a href="#main-content">Skip to main content</a></div>
<div id="pst-scroll-pixel-helper"></div>
<button class="btn rounded-pill" id="pst-back-to-top" type="button">
<i class="fa-solid fa-arrow-up"></i>Back to top</button>
<input class="sidebar-toggle" id="pst-primary-sidebar-checkbox" type="checkbox"/>
<label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
<input class="sidebar-toggle" id="pst-secondary-sidebar-checkbox" type="checkbox"/>
<label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
<div class="search-button__wrapper">
<div class="search-button__overlay"></div>
<div class="search-button__search-container">
<form action="../../../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search this book..." autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search this book..." spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
</div>
<div class="pst-async-banner-revealer d-none">
<aside aria-label="Version warning" class="d-none d-print-none" id="bd-header-version-warning"></aside>
</div>
<header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
</header>
<div class="bd-container">
<div class="bd-container__inner bd-page-width">
<div class="bd-sidebar-primary bd-sidebar">
<div class="sidebar-header-items sidebar-primary__section">
</div>
<div class="sidebar-primary-items__start sidebar-primary__section">
<div class="sidebar-primary-item">
<a class="navbar-brand logo" href="../../../landing-page.html">
<img alt="BoBiAC Book - Home" class="logo__image only-light" src="../../../_static/bobiac_logos_svgexport-03.svg"/>
<script>document.write(`<img src="../../../_static/bobiac_logos_svgexport-04.svg" class="logo__image only-dark" alt="BoBiAC Book - Home"/>`);</script>
</a></div>
<div class="sidebar-primary-item">
<script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
<div class="sidebar-primary-item"><nav aria-label="Main" class="bd-links bd-docs-nav">
<div class="bd-toc-item navbar-nav active">
<ul class="nav bd-sidenav bd-sidenav__home-link">
<li class="toctree-l1">
<a class="reference internal" href="../../../landing-page.html">
<i class="fas fa-home"></i> Home
                </a>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Course Material</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../01_intro_to_bobiac/bobiac_intro.html">01 - <i class="fas fa-image"></i> Introduction to BoBiAC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../02_getting_started_with_python/getting_started_with_python.html">02 - <i class="fab fa-python"></i> Getting Started with Python and <code class="docutils literal notranslate"><span class="pre">uv</span></code></a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../03_python_basics/python_basics.html">03 - <i class="fab fa-python"></i> Python Basics</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../03_python_basics/python_basics_notebook.html">The Python Basics Notebook</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../04_digital_images_intro/digital_images_intro.html">04 - <i class="fas fa-table-cells"></i> Introduction to Digital Images</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../04_digital_images_intro/python_for_bioimage_analysis.html">Python for bioimage analysis</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../segmentation_intro.html">05 - <i class="fa-solid fa-disease"></i> Image Segmentation</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2 has-children"><a class="reference internal" href="../classic/classic.html">Classical Methods</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../classic/classic_segmentation.html">Classical Segmentation</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../machine_learning/machine_learning.html">Machine Learning Methods</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../machine_learning/intro_to_ilastik.html">Introduction to Ilastik</a></li>
<li class="toctree-l3"><a class="reference internal" href="../machine_learning/pixel_classification_with_ilastik.html">Ilastik for Pixel Classification</a></li>
<li class="toctree-l3"><a class="reference internal" href="../machine_learning/from_ilastik_masks_to_labels.html">From Ilastik Masks to Labels</a></li>
</ul>
</details></li>
<li class="toctree-l2 current active has-children"><a class="current reference internal" href="#">Deep Learning Methods</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="intro_to_cellpose.html">Introduction to Cellpose</a></li>
<li class="toctree-l3"><a class="reference internal" href="cellpose_notebook.html">Cellpose in Python</a></li>
<li class="toctree-l3"><a class="reference internal" href="cellpose_retraining_colab.html">Retraining Cellpose on Custom Data</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../06_object_classification/object_classification.html">06 - <i class="fa-solid fa-shapes"></i> Object Classification</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../06_object_classification/object_classification_with_ilastik.html">Ilastik for Object Classification</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../07_measurement_and_quantification/measurement_and_quantification_intro.html">07 - <i class="fa-solid fa-chart-simple"></i> Measurements &amp; Quantification</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../07_measurement_and_quantification/measurement_and_quantification_notebook.html">Measurements &amp; Quantification Notebook</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../07_measurement_and_quantification/background_correction_notebook.html">Background Correction</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../08_colocalization/colocalization_intro.html">08 - <i class="fa-solid fa-location-crosshairs"></i> Colocalization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../08_colocalization/pixel_intensity_based_colocalization_pearsons.html">Pearson’s correlation coefficient</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../08_colocalization/pixel_intensity_based_colocalization_manders.html">Manders’ Correlation Coefficients</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../08_colocalization/object_based_colocalization.html">Object-based colocalization analysis</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../09_reproducibility_and_image_ethics/reproducibility_and_image_ethics.html">09 - <i class="fa-solid fa-rotate-right"></i> Reproducibility and Image Ethics</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Student Working Groups</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../student_group_work/student_group_work.html"><i class="fa-solid fa-user-group"></i> Student Group Work</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../student_group_work/evaluation-notebook.html">Evaluation Notebook</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Course Materials Downloads</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../data/course_downloads.html"><i class="fa-solid fa-folder"></i> Downloads</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Links</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="https://iac.hms.harvard.edu/bobiac/2025">BoBiAC</a></li>
<li class="toctree-l1"><a class="reference external" href="https://docs.astral.sh/uv/">uv</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/manzt/juv">juv</a></li>
<li class="toctree-l1"><a class="reference external" href="https://iac.hms.harvard.edu">Image Analysis Collaboratory (IAC)</a></li>
<li class="toctree-l1"><a class="reference external" href="https://cite.hms.harvard.edu">Core for Imaging Technology &amp; Education (CITE)</a></li>
</ul>
</div>
</nav></div>
</div>
<div class="sidebar-primary-items__end sidebar-primary__section">
</div>
<div id="rtd-footer-container"></div>
</div>
<main class="bd-main" id="main-content" role="main">
<div class="sbt-scroll-pixel-helper"></div>
<div class="bd-content">
<div class="bd-article-container">
<div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
<div class="header-article-items__start">
<div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" data-bs-placement="bottom" data-bs-toggle="tooltip" title="Toggle primary sidebar">
<span class="fa-solid fa-bars"></span>
</button></div>
</div>
<div class="header-article-items__end">
<div class="header-article-item">
<div class="article-header-buttons">
<a class="btn btn-sm btn-source-repository-button" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://github.com/HMS-IAC/bobiac" target="_blank" title="Source repository">
<span class="btn__icon-container">
<i class="fab fa-github"></i>
</span>
</a>
<button class="btn btn-sm btn-fullscreen-button" data-bs-placement="bottom" data-bs-toggle="tooltip" onclick="toggleFullScreen()" title="Fullscreen mode">
<span class="btn__icon-container">
<i class="fas fa-expand"></i>
</span>
</button>
<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>
<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" data-bs-placement="bottom" data-bs-toggle="tooltip" title="Toggle secondary sidebar">
<span class="fa-solid fa-list"></span>
</button>
</div></div>
</div>
</div>
</div>
<div class="onlyprint" id="jb-print-docs-body">
<h1>Deep Learning Methods</h1>
<!-- Table of contents -->
<div id="print-main-content">
<div id="jb-print-toc">
<div>
<h2 style="color: black; background-color: rgb(127,196,125); padding: 3px; border-radius: 5px;"> Contents </h2>
</div>
<nav aria-label="Page">
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-netwok">Neural Netwok</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-neural-network">What is a Neural Network?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-does-a-neural-network-learn">How Does a Neural Network Learn?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#convolutional-neural-network-cnn-2010">Convolutional Neural Network (CNN) [2010 - ]</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-cnn">What is a CNN?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#segmentation-as-a-pixel-classifier">Segmentation as a Pixel Classifier</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introducing-u-net-2015">Introducing U-Net [2015]</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-features">Key Features</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-u-net-works-so-well-for-bioimages">Why U-Net Works So Well for Bioimages:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cellpose-2020-a-generalist-model-using-flow-fields">Cellpose [2020]: A Generalist Model Using Flow Fields</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#whats-new-in-cellpose">What’s New in Cellpose</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#advantages-of-cellpose">Advantages of Cellpose</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transformers-and-vision-transformers-2019">Transformers and Vision Transformers [2019 - ]</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#segment-anything-model-sam-2023">Segment Anything Model (SAM) [2023]</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cellposesam-2024">CellposeSAM [2024]</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#whats-new-in-cellposesam">What’s New in CellposeSAM</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-it-matters">Why It Matters</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
</ul>
</nav>
</div>
</div>
</div>
<div id="searchbox"></div>
<article class="bd-article">
<section class="tex2jax_ignore mathjax_ignore" id="deep-learning-methods">
<h1>Deep Learning Methods<a class="headerlink" href="#deep-learning-methods" title="Link to this heading">#</a></h1>
<p>Deep learning is a branch of machine learning that takes things a step further. Instead of relying on manually selected features (like edges or textures), deep learning models learn to extract <strong>their own features</strong> directly from raw image data. This makes them especially powerful for complex tasks like image segmentation.</p>
<div align="center">
<img alt="dl" src="../../../_static/images/dl/seg_methods_dl.png" width="700"/>
</div>
<br/>
<p>In a typical deep learning workflow, we feed the entire image into a <strong>neural network</strong>, a multi-layered model inspired by the brain. Through training on many labeled examples, the network learns to recognize patterns, structures, and shapes in the image. Each layer in the network transforms the data a little more, gradually building up an understanding of what’s in the image.</p>
<p>Unlike traditional machine learning, we don’t need to tell the model <em>what</em> to look for—it figures that out on its own.</p>
<p>In <strong>image segmentation</strong>, deep learning models assign a class to each pixel (e.g., cell, background, nucleus) and output a full <strong>segmentation mask</strong>. These masks can capture fine details and handle challenging cases like overlapping cells, blurry edges, or varying lighting conditions.</p>
<p>Because deep learning learns directly from data and can model very complex relationships, it often outperforms classical and machine learning methods for robustness, however, this performance comes at a higher cost of training and data requirement.</p>
<hr class="docutils"/>
<section id="neural-netwok">
<h2 style="color: black; background-color: rgb(127,196,125); padding: 3px; border-radius: 5px;">Neural Netwok<a class="headerlink" href="#neural-netwok" title="Link to this heading">#</a></h2>
<section id="what-is-a-neural-network">
<h3 style="color: black; background-color: rgb(190,223,185); padding: 3px; border-radius: 5px;">What is a Neural Network?<a class="headerlink" href="#what-is-a-neural-network" title="Link to this heading">#</a></h3>
<p>A neural network is a mathematical model inspired by how the brain works. It is made up of layers of units called neurons, which are connected to each other and pass information forward.</p>
<p>Each neuron performs a very simple task:</p>
<ol class="arabic simple">
<li><p>It takes in numbers as input.</p></li>
<li><p>It combines them in a weighted way (like multiplying by importance).</p></li>
<li><p>It passes the result through a simple function.</p></li>
<li><p>It sends this value to the next layer of neurons.</p></li>
</ol>
<p>If you stack many such layers on top of each other, you get a deep neural network—hence the term deep learning.</p>
<div align="center">
<img alt="ANN" src="../../../_static/images/deep_learning/ann.png" width="700"/>
</div>
<p><em>Source: <a class="reference external" href="https://towardsdatascience.com/the-concept-of-artificial-neurons-perceptrons-in-neural-networks-fab22249cbfc/">https://towardsdatascience.com/the-concept-of-artificial-neurons-perceptrons-in-neural-networks-fab22249cbfc/</a></em></p>
</section>
<section id="how-does-a-neural-network-learn">
<h3 style="color: black; background-color: rgb(190,223,185); padding: 3px; border-radius: 5px;">How Does a Neural Network Learn?<a class="headerlink" href="#how-does-a-neural-network-learn" title="Link to this heading">#</a></h3>
<p>Neural networks learn by example, not by rules.</p>
<p>Here’s how it works:</p>
<ol class="arabic simple">
<li><p>You give the model an image (e.g., of a cell) and the correct answer (a labeled segmentation mask).</p></li>
<li><p>The model makes a guess (its prediction).</p></li>
<li><p>It compares its guess to the correct answer using a loss function (a measure of error).</p></li>
<li><p>It updates its internal settings (weights) to do better next time.</p></li>
</ol>
<p>This process is called training, and it’s repeated over thousands of examples until the network gets good at making predictions.</p>
<!-- ### What Makes Deep Learning Different?
Traditional algorithms follow explicit instructions that we write. For example:

- “If a pixel is brighter than 100, mark it as foreground.”
- “Apply a Gaussian blur, then find the edges.”

Deep learning, in contrast, learns its own instructions. It creates internal representations of features in the image and decides for itself how to best solve the task. This is powerful because:

- It can handle noise and variability.
- It finds patterns we might miss.
- It doesn’t need us to handcraft every rule.

**Key idea:** *We don’t program deep learning models—we train them.* -->
</section>
</section>
<hr class="docutils"/>
<section id="convolutional-neural-network-cnn-2010">
<h2 style="color: black; background-color: rgb(127,196,125); padding: 3px; border-radius: 5px;">Convolutional Neural Network (CNN) [2010 - ]<a class="headerlink" href="#convolutional-neural-network-cnn-2010" title="Link to this heading">#</a></h2>
</section>
<section id="what-is-a-cnn">
<h2 style="color: black; background-color: rgb(127,196,125); padding: 3px; border-radius: 5px;">What is a CNN?<a class="headerlink" href="#what-is-a-cnn" title="Link to this heading">#</a></h2>
<p>A convolutional neural network (CNN) is a type of neural network specially designed for images.</p>
<p>Key Ideas in CNNs:</p>
<ul class="simple">
<li><p><strong>Convolutional layers:</strong> Filters (or “kernels”) slide over the image and detect local patterns like edges, lines, or textures.</p></li>
<li><p><strong>Pooling layers:</strong> Downsample the image to retain important information while reducing size.</p></li>
<li><p><strong>Stacked layers:</strong> Combine many layers so the network can build up from simple features to complex ones (e.g., from edges to full cell shapes).</p></li>
</ul>
<div align="center">
<img alt="CNN" src="../../../_static/images/deep_learning/cnn.jpg" width="700"/>
</div>
<p><em>Source: <a class="reference external" href="https://medium.com/@RaghavPrabhu/understanding-of-convolutional-neural-network-cnn-deep-learning-99760835f148">https://medium.com/@RaghavPrabhu/understanding-of-convolutional-neural-network-cnn-deep-learning-99760835f148</a></em></p>
<p>CNNs became popular after the success of <strong>AlexNet in 2012</strong>, which won the ImageNet competition by a large margin. This moment marked the beginning of the modern deep learning era.</p>
</section>
<hr class="docutils"/>
<section id="segmentation-as-a-pixel-classifier">
<h2 style="color: black; background-color: rgb(127,196,125); padding: 3px; border-radius: 5px;">Segmentation as a Pixel Classifier<a class="headerlink" href="#segmentation-as-a-pixel-classifier" title="Link to this heading">#</a></h2>
<p>While CNNs were first used for image classification (e.g., “Does this image contain a cat?”), biomedical researchers needed more: per-pixel predictions like “Which pixels belong to which cell?”</p>
<p>Early solutions tried to adapt classification networks into segmentation tasks by:</p>
<ul class="simple">
<li><p>Applying CNNs to small image patches</p></li>
<li><p>Upsampling coarse predictions</p></li>
</ul>
<p>But this was inefficient and inaccurate for fine biological structures.</p>
<div align="center">
<img alt="SegNet" src="../../../_static/images/deep_learning/segnet.png" width="700"/>
</div>
<p><em>Source: SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation</em></p>
</section>
<hr class="docutils"/>
<section id="introducing-u-net-2015">
<h2 style="color: black; background-color: rgb(127,196,125); padding: 3px; border-radius: 5px;">Introducing U-Net [2015]<a class="headerlink" href="#introducing-u-net-2015" title="Link to this heading">#</a></h2>
<p>The real breakthrough for biomedical image segmentation came with the introduction of U-Net in 2015 by Ronneberger et al., developed specifically for segmenting cells in light microscopy images.</p>
<section id="key-features">
<h3 style="color: black; background-color: rgb(190,223,185); padding: 3px; border-radius: 5px;">Key Features<a class="headerlink" href="#key-features" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Component</p></th>
<th class="head"><p>Function</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Encoder (contracting path)</strong></p></td>
<td><p>Learns what features are present. Downsamples the image while extracting key patterns.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Decoder (expanding path)</strong></p></td>
<td><p>Reconstructs the segmentation mask. Upsamples to original image size.</p></td>
</tr>
<tr class="row-even"><td><p><strong>Skip connections</strong></p></td>
<td><p>Link encoder and decoder layers to preserve spatial details like edges and boundaries.</p></td>
</tr>
</tbody>
</table>
</div>
<div align="center">
<img alt="UNet" src="../../../_static/images/deep_learning/unet.png" width="700"/>
</div>
<p><em>Source: U-Net: Convolutional Networks for Biomedical Image Segmentation</em></p>
</section>
<section id="why-u-net-works-so-well-for-bioimages">
<h3 style="color: black; background-color: rgb(190,223,185); padding: 3px; border-radius: 5px;">Why U-Net Works So Well for Bioimages:<a class="headerlink" href="#why-u-net-works-so-well-for-bioimages" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Works well even with small datasets.</p></li>
<li><p>Preserves fine details through skip connections.</p></li>
<li><p>Generalizes across different staining protocols and microscopy types.</p></li>
<li><p>Can be trained end-to-end from raw images to segmentation masks.</p></li>
</ul>
</section>
</section>
<hr class="docutils"/>
<section id="cellpose-2020-a-generalist-model-using-flow-fields">
<h2 style="color: black; background-color: rgb(127,196,125); padding: 3px; border-radius: 5px;">Cellpose [2020]: A Generalist Model Using Flow Fields<a class="headerlink" href="#cellpose-2020-a-generalist-model-using-flow-fields" title="Link to this heading">#</a></h2>
<p>While U-Net was powerful, it sometimes struggled with complex shapes or overlapping cells. <strong>Cellpose</strong> took segmentation further by introducing flow fields, a way for the model to understand the shape and direction of each object.</p>
<p><strong>Cellpose</strong> is designed to be a <em>generalist model</em> for microscopy cell-segmentation. Cellpose doesn’t just classify pixels; it learns how pixels move together to form a whole cell. That made it incredibly flexible, able to segment nuclei, cytoplasm, or even non-biological shapes, often without retraining.</p>
<section id="whats-new-in-cellpose">
<h3 style="color: black; background-color: rgb(190,223,185); padding: 3px; border-radius: 5px;">What’s New in Cellpose<a class="headerlink" href="#whats-new-in-cellpose" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Introduces a novel idea: <strong>flow fields</strong></p>
<ul>
<li><p>For each pixel, the model predicts the direction it should move to reach the object center.</p></li>
<li><p>After computing this flow, pixels are grouped into objects.</p></li>
</ul>
</li>
<li><p>Trained on a diverse set of cell and tissue types—generalist approach.</p></li>
<li><p>Works out of the box on many image types, without retraining.</p></li>
</ul>
<div align="center">
<img alt="Cellpose" src="../../../_static/images/deep_learning/cellpose-overlay.webp" width="700"/>
</div>
<p><em>Source: Cellpose: a generalist algorithm for cellular segmentation</em></p>
</section>
<section id="advantages-of-cellpose">
<h3 style="color: black; background-color: rgb(190,223,185); padding: 3px; border-radius: 5px;">Advantages of Cellpose<a class="headerlink" href="#advantages-of-cellpose" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Doesn’t assume round shapes.</p></li>
<li><p>Segments cytoplasm, cells, or nuclei—flexible across tasks.</p></li>
<li><p>Minimal parameter tuning needed.</p></li>
<li><p>Provides segmentation even on images with no close training data.</p></li>
</ul>
</section>
</section>
<hr class="docutils"/>
<section id="transformers-and-vision-transformers-2019">
<h2 style="color: black; background-color: rgb(127,196,125); padding: 3px; border-radius: 5px;">Transformers and Vision Transformers [2019 - ]<a class="headerlink" href="#transformers-and-vision-transformers-2019" title="Link to this heading">#</a></h2>
<p>In recent years, deep learning has gone through another big shift with the introduction of <strong>transformers</strong>—a model architecture originally developed for natural language processing (like in translation or chatbots).</p>
<p>Unlike CNNs, which focus on local patterns using small filters, transformers are designed to capture <strong>global context</strong>. That means they can see the “big picture” in an image and understand how different parts relate to each other—even if they’re far apart.</p>
<p>In 2020, researchers began adapting transformers to image analysis, calling them <strong>Vision Transformers (ViT)</strong>. These models cut the image into patches (like puzzle pieces) and learn relationships between them, rather than sliding a filter across the entire image like CNNs.</p>
<div align="center">
<img alt="ViT" src="../../../_static/images/deep_learning/vit.png" width="700"/>
</div>
<p><em>Source: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</em></p>
<p>Vision transformers are particularly good at handling:</p>
<ul class="simple">
<li><p>Complex textures or shapes</p></li>
<li><p>Large images with multiple objects</p></li>
<li><p>Tasks that benefit from global attention (like segmentation with sparse guidance)</p></li>
</ul>
</section>
<hr class="docutils"/>
<section id="segment-anything-model-sam-2023">
<h2 style="color: black; background-color: rgb(127,196,125); padding: 3px; border-radius: 5px;">Segment Anything Model (SAM) [2023]<a class="headerlink" href="#segment-anything-model-sam-2023" title="Link to this heading">#</a></h2>
<p>One of the most talked-about applications of transformers in computer vision is the <strong>Segment Anything Model (SAM)</strong> from Meta AI. As the name suggests, SAM was built to <strong>segment any object in any image</strong>, given a simple prompt like a point, a box, or a rough scribble.</p>
<p>SAM is not specific to microscopy—but it’s powerful. It combines the flexibility of transformers with a unique prompt-based workflow. That means:</p>
<ul class="simple">
<li><p>You don’t need a full set of labels to segment something.</p></li>
<li><p>You can just click on an image to guide the model.</p></li>
<li><p>It works well across domains, from natural scenes to X-rays.</p></li>
</ul>
<div align="center">
<img alt="SAM" src="../../../_static/images/deep_learning/sam.png" width="700"/>
</div>
<p><em>Source: Segment Anything by Meta AI</em></p>
</section>
<hr class="docutils"/>
<section id="cellposesam-2024">
<h2 style="color: black; background-color: rgb(127,196,125); padding: 3px; border-radius: 5px;">CellposeSAM [2024]<a class="headerlink" href="#cellposesam-2024" title="Link to this heading">#</a></h2>
<p>While SAM is impressive, it wasn’t originally designed for the challenges of microscopy: fuzzy boundaries, overlapping cells, weak contrast, or varying staining. That’s where <strong>CellposeSAM</strong> comes in.</p>
<p><strong>CellposeSAM</strong> combines the <strong>generalist power of SAM</strong> with the <strong>domain-specific strengths of Cellpose</strong>, creating a flexible tool for <strong>interactive cell segmentation</strong>. Think of it as the best of both worlds.</p>
<section id="whats-new-in-cellposesam">
<h3 style="color: black; background-color: rgb(190,223,185); padding: 3px; border-radius: 5px;">What’s New in CellposeSAM<a class="headerlink" href="#whats-new-in-cellposesam" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Uses <strong>SAM’s transformer-based segmentation head</strong> for high-quality boundaries.</p></li>
<li><p>Integrates <strong>Cellpose’s cell-aware flow fields</strong> for better handling of microscopy-specific structures.</p></li>
<li><p>Supports <strong>interactive prompts</strong> like clicks or masks to guide segmentation.</p></li>
</ul>
<div align="center">
<img alt="CellposeSAM" src="../../../_static/images/deep_learning/cellpose_sam_fig1.jpg" width="700"/>
</div>
<p><em>Source: CellposeSAM (2024), Broad Institute / Chan Zuckerberg Initiative</em></p>
</section>
<section id="why-it-matters">
<h3 style="color: black; background-color: rgb(190,223,185); padding: 3px; border-radius: 5px;">Why It Matters<a class="headerlink" href="#why-it-matters" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Easy to interact with: Click a point, get a mask.</p></li>
<li><p>Works on many image types: Brightfield, fluorescence, tissues, and more.</p></li>
<li><p>Less manual work, more accuracy.</p></li>
</ul>
</section>
</section>
<hr class="docutils"/>
<section id="summary">
<h2 style="color: black; background-color: rgb(127,196,125); padding: 3px; border-radius: 5px;">Summary<a class="headerlink" href="#summary" title="Link to this heading">#</a></h2>
<p>Over the past decade, we’ve gone from:</p>
<ul class="simple">
<li><p><strong>Hand-crafted features</strong> (machine learning),</p></li>
<li><p>to <strong>image-specific models</strong> (CNNs),</p></li>
<li><p>to <strong>segmentation models</strong> <strong>U-Net</strong></p></li>
<li><p>to <strong>generalist architectures</strong> for cell-segmentation <strong>Cellpose</strong>,</p></li>
<li><p>and now to <strong>promptable, transformer-powered models</strong> like <strong>SAM and CellposeSAM</strong>.</p></li>
</ul>
<p>In the next section, we’ll explore how to use <strong>CellposeSAM</strong> in practice.</p>
</section>
<div class="toctree-wrapper compound">
</div>
</section>
<script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./content/05_segmentation/deep_learning"
        },
        predefinedOutput: true
    }
    </script>
<script>kernelName = 'python3'</script>
</article>
<footer class="prev-next-footer d-print-none">
<div class="prev-next-area">
<a class="left-prev" href="../machine_learning/from_ilastik_masks_to_labels.html" title="previous page">
<i class="fa-solid fa-angle-left"></i>
<div class="prev-next-info">
<p class="prev-next-subtitle">previous</p>
<p class="prev-next-title">From Ilastik Masks to Labels</p>
</div>
</a>
<a class="right-next" href="intro_to_cellpose.html" title="next page">
<div class="prev-next-info">
<p class="prev-next-subtitle">next</p>
<p class="prev-next-title">Introduction to Cellpose</p>
</div>
<i class="fa-solid fa-angle-right"></i>
</a>
</div>
</footer>
</div>
<div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">
<div class="sidebar-secondary-item">
<div class="page-toc tocsection onthispage">
<i class="fa-solid fa-list"></i> Contents
  </div>
<nav class="bd-toc-nav page-toc">
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-netwok">Neural Netwok</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-neural-network">What is a Neural Network?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-does-a-neural-network-learn">How Does a Neural Network Learn?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#convolutional-neural-network-cnn-2010">Convolutional Neural Network (CNN) [2010 - ]</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-cnn">What is a CNN?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#segmentation-as-a-pixel-classifier">Segmentation as a Pixel Classifier</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introducing-u-net-2015">Introducing U-Net [2015]</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-features">Key Features</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-u-net-works-so-well-for-bioimages">Why U-Net Works So Well for Bioimages:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cellpose-2020-a-generalist-model-using-flow-fields">Cellpose [2020]: A Generalist Model Using Flow Fields</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#whats-new-in-cellpose">What’s New in Cellpose</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#advantages-of-cellpose">Advantages of Cellpose</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transformers-and-vision-transformers-2019">Transformers and Vision Transformers [2019 - ]</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#segment-anything-model-sam-2023">Segment Anything Model (SAM) [2023]</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cellposesam-2024">CellposeSAM [2024]</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#whats-new-in-cellposesam">What’s New in CellposeSAM</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-it-matters">Why It Matters</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
</ul>
</nav></div>
</div></div>
</div>
<footer class="bd-footer-content">
<div class="bd-footer-content__inner container">
<div class="footer-item">
<p class="component-author">
By Federico Gasparoli - federico.gasparoli@gmail.com, Image Analysis Collaboratory @ Harvard Medical School
</p>
</div>
<div class="footer-item">
<p class="copyright">
    
      © Copyright 2025.
      <br>
</br></p>
</div>
<div class="footer-item">
</div>
<div class="footer-item">
<div class="extra_footer">
<p>
All content is licensed under <a href="https://creativecommons.org/licenses/by/4.0/" target="_blank">CC-BY 4.0</a>, except where noted otherwise.
</p>
</div>
</div>
</div>
</footer>
</main>
</div>
</div>
<!-- Scripts loaded after <body> so the DOM is not blocked -->
<script src="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>
<footer class="bd-footer">
</footer>
</body>
</html>